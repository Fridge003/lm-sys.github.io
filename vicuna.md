---
layout: default
title: "Vicuna: An Open Recipe to Reach 90% ChatGPT Quality and Impress GPT-4."
description: by the Team with members from UC Berkeley, CMU, Stanford, and UC San Diego
---

## TL;DR
Introducing Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMa on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves 90% quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMa and Stanford Alpaca in more than 90% of cases. The cost of training Vicuna-13B is around $300. The training and serving [code](), along with an online [demo](), are publicly available for non-commercial use.

## Sample Output
Here is an example of our evaluation. We ask the two models the same question and let GPT-4 assess the response quality. GPT-4 favors the response generated by Vicuna-13B over Alpaca-13B in this case.

Similarly, we evaluate five models on a set of 80 challenging benchmark questions generated by GPT-4, These questions span various topics, such as common-sense, counterfactuals, roleplay, writing, and coding. With careful prompt engineering, we are able to ask GPT-4 to examine the quality of answers from each model and rate overall scores. More details are provided in the evaluation section. We summarize the relative scores in the figure below, which shows that Vicuna-13B achieves 90% quality compared to ChatGPT.

## Online Demo

[a gif screenshot]

## Overview
The rapid advancement of large language models (LLMs) has revolutionized chatbot systems, resulting in unprecedented levels of intelligence as seen in OpenAI's ChatGPT. However, despite its impressive performance, the training and architecture details of ChatGPT remain unclear, hindering research and open-source innovation in this field. Inspired by the Meta LLaMa and Stanford Alpaca project, we introduce Vicuna-13B, an open-source chatbot backed by an enhanced dataset and an easy-to-use, scalable infrastructure. By fine-tuning a LLaMa base model on user-shared conversations collected from ShareGPT.com, Vicuna-13B has demonstrated competitive performance compared to other open-source models like Stanford Alpaca. This blog post provides a preliminary evaluation of Vicuna-13B's performance and describes its training and serving infrastructure. We also invite the community to interact with our online demo to test the capabilities of this chatbot.

![Workflow](assets/vicuna/workflow.png){: style="display:block; margin-left: auto; margin-right: auto;" width="500"}
Figure 1. Workflow Overview
{: style="color:gray; font-size: 80%; text-align: center;"}

The above figure provides an overview of our work. To begin, we collected around 70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations. Next, we enhanced the training scripts provided by Alpaca to better handle multi-round conversations and long sequences. The training was done with PyTorch FSDP on 8 A100 GPUs in one day. For serving the demo, we implemented a lightweight distributed serving system. We conducted a preliminary evaluation of the model quality by creating a set of 80 diverse questions and utilizing GPT-4 to judge the model outputs. To compare two different models, we combine the outputs from each model into a single prompt for each question. The prompts are then sent to GPT-4, which assesses which model provided better responses. A detailed comparison of LLaMa, Alpaca, ChatGPT, and Vicuna is shown in the table below.

## Training
Vicuna is created by fine-tuning a LLaMa base model using approximately 70K user-shared conversations gathered from ShareGPT.com. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model's maximum context length.

Our training recipe builds on top of Stanfordâ€™s alpaca with the following improvements.
- Memory Optimizations: To enable Vicuna's understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing gradient checkpointing and flash attention.
- Multi-round conversations. We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot's output.
- Cost Reduction via Spot Instance: The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses.We employ SkyPilot managed spot to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500  to around $140 and 13B model from around $1K to $300.

## Serving
We build a serving system that is capable of serving multiple models with distributed workers. The architecture of the system is shown below. It supports flexible plug-in of GPU workers from both on-premise clusters and the cloud. By utilizing a fault-tolerant controller and managed spot instance feature in SkyPilot, this serving system can work well with spot instances from multiple clouds to reduce the serving costs. It is currently a lightweight implementation and we are working on integrating more of our latest research into it.

![Workflow](assets/vicuna/serving_arch.jpg){: style="display:block; margin-left: auto; margin-right: auto;" width="500"}

Figure 2. Serving System Architecture
{: style="color:gray; font-size: 80%; text-align: center;"}


## License
The online demo is a research preview intended for non-commercial use only, subject to the model License of LLaMa and Terms of Use of the data generated by OpenAI. Please contact us If you find any potential violation.
The code is released under the Apache License 2.0.

## The Team
This is a joint effort with collaborators from multiple institutions, including UC Berkeley, CMU, Stanford, and UC San Diego.  

**Students in alphabetical order**:  
Weilin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang

## Acknowledgment
We would like to thank Xinyang Geng, Hao Liu, and Eric Wallace for their insightful discussion and feedback. Please check out another concurrent [effort]() from BAIR.
