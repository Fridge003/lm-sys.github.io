---
title: "Cutting LLM Costs Without Compromising Quality with RouteLLM"
author: "Isaac Ong*, Amjad Almahairi*, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, M Waleed Kadous, Ion Stoica"
date: "July 1, 2024"
previewImg: /images/blog/routellm/cover.png
---

LLMs have demonstrated remarkable capabilities across a range of tasks, but there exists wide variation in the costs and capabilities of different LLMs, as seen from a plot of performance against cost of LLMs in Figure 1. Very broadly, more capable models tend to be more expensive than less capable models. This leads to a dilemma when deploying LLMs in the real-world - routing all queries to the largest, most capable model leads to the highest-quality responses but can be expensive, while routing queries to smaller models can save costs but may result in lower-quality responses.

<img src="/images/blog/routellm/main.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 50%"></img>

<p style="color:gray; text-align: center;">Figure 1: Plot of performance against cost of various LLMs. Performance is measured by Elo on Chatbot Arena, and cost per million tokens assuming a 1:1 input / output ratio. Through routing between two models, we ideally achieve a better performance:cost ratio than can be achieved with either model.</p>

LLM routing offers a solution to this problem, whereby each query is first processed by a system that decides which LLM to route it to. Ideally, the system should route all queries that can be sufficiently handled by weaker models to such models, and all other queries to stronger models, minimizing cost while maintaining response quality. However, this turns out to be a challenging problem because the routing system has to infer both the characteristics of an incoming query and different models’ capabilities before routing.

To tackle this, we present **RouteLLM**, a principled framework for LLM routing using preference data. We formalize the problem of LLM routing and explore techniques to understand and improve router performance. Using these ideas, we trained four different routers using data from Chatbot Arena and demonstrate that they can significantly reduce costs without compromising quality on widely-used benchmarks, with **cost reductions of up to 70% on MT Bench, 30% on MMLU, and 40% on GSM8K** for the same performance relative to our baselines. We also release our code and datasets publicly, including a new [open-source library](https://github.com/lm-sys/RouteLLM) for serving and evaluating LLM routers.

## Routing Setup

In our routing setup, we focus on the case where there are two models: a stronger, more expensive model, and a weaker but cheaper model. Given this setup, our objective is to minimize costs while achieving high quality by routing between both models.

<img src="/images/blog/routellm/metrics.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 45%"></img>


<p style="color:gray; text-align: center;">Figure 2: Random router performance on MT Bench</p>

This is best understood through Figure 2, which represents the performance of a router that randomly routes between the two models on MT Bench. Specifically, we route between GPT-4 and Mixtral 8x7B here, with the performance of these two models denoted by the red and grey dotted lines. Similar to this, for any router, we can plot a graph of its performance against the number of the calls made to GPT-4 (which is representative of the cost incurred since GPT-4 cost is dominant).

To train our routers, we use *preference data*, which each consists of a prompt and a comparison between the response quality of two models on that prompt i.e. this could be a win for the first model, a win for the second model, or a tie. Using preference data allows us to learn about the strengths and weaknesses of different models and how they relate to user queries, which we believe is effective for training routers. For our base dataset, we utilize data obtained from [Chatbot Arena](http://chat.lmsys.org) of approximately 80,000 battles between different models. We also investigate *data augmentation* to further improve routing performance by expanding the pool of data using both golden-label datasets and a LLM judge.

We trained four routers using a mix of preference data obtained from Chatbot Arena and data augmentation:
- A similarity-weighted (SW) ranking router that performs a “weighted Elo calculation” drawing inspiration from Chatbot Arena
- A matrix factorization model that learns a scoring function for how well a model can answer a prompt
- A BERT classifier that predicts which model can provide a better response for a prompt
- A causal LLM classifier that also predicts which model can provide a better response for a prompt

## Results

We evaluated these routers on three popular benchmarks: [MT Bench](https://arxiv.org/abs/2306.05685), [MMLU](https://arxiv.org/abs/2009.03300), and [GSM8K](https://arxiv.org/abs/2110.14168), presenting results for MT Bench and MMLU below. For evaluation, we route between `gpt-4-1106-preview` as our strong model and Mixtral 8x7B as our weak model. We use the random router from before as our baseline.

<br />
<figure style="text-align: center">
<img src="/images/blog/routellm/unaugmented-mt-bench.png" style="display:inline; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 45%"></img>
<img src="/images/blog/routellm/augmented-mt-bench.png" style="display:inline; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 45%"></img>
</figure>

<p style="color:gray; text-align: center;">Figure 3: Router performance on MT Bench (left) trained only on Arena data. (right) trained on Arena data augmented using a LLM judge.</p>

Figure 3 displays the performance of our routers on MT Bench. For routers trained only on the Arena dataset, we observe strong performance for both matrix factorization and SW ranking, with both routers performing better than the random router across all metrics. Notably, matrix factorization is able to achieve 50% of the performance gap between GPT-4 and Mixtral with an approximately 50% cost reduction as compared to the baseline.

We also augmented the Arena data using an LLM judge. Doing so leads to significant improvements across all routers when trained on this data. On this augmented dataset, matrix factorization is again the best-performing router, with the cost required to achieve 50% of the performance gap further halved, meaning that we are able to achieve the same performance as our random baseline with only about 25% of the cost.

<br />
<figure style="text-align: center">
<img src="/images/blog/routellm/unaugmented-mmlu.png" style="display:inline; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 45%"></img>
<img src="/images/blog/routellm/augmented-mmlu.png" style="display:inline; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 45%"></img>
</figure>


<p style="color:gray; text-align: center;">Figure 4: Router performance on MMLU (left) trained only on Arena data. (right) trained on Arena data augmented using golden-label data from the MMLU validation split.</p>

Conversely, on MMLU in Figure 4, all routers perform poorly at a near-random level when trained only on Arena dataset, which we attribute to most MMLU questions being out-of-distribution. However, augmenting the training dataset using golden-label data from the MMLU validation split leads to significant performance improvements across all routers, with our best router now requiring nearly 30% less cost relative to the baseline to achieve 50% of the performance gap between GPT-4 and Mixtral. Importantly, this golden-labeled dataset of approximately 1500 samples represents less than 2% of the overall training data, demonstrating the effectiveness of data augmentation even when the number of samples is small.

### Industry Benchmarks

<br />
<img src="/images/blog/routellm/indep-benchmarks.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 45%"></img>

<p style="color:gray; text-align: center;">Figure 6: Comparison of our router against existing routing systems on MT Bench.</p>

In Figure 6, we also report the performance of our best-performing routers on MT Bench against [Unify AI](https://unify.ai/) and [Not Diamond](https://www.notdiamond.ai/notdiamond-0001), two existing LLM routing systems released by companies. Our routers demonstrate competitive performance, achieving the same performance as other available routers while being about 40% cheaper.

### Generalizing to Other Models

While we pick GPT-4 and Mixtral as representative strong and weak models for the above results, we also experiment with a different model pair to demonstrate the generalizability of our framework. Figure 7 shows the MT Bench results when we replace our model pair with Claude 3 Opus and Llama 3 8B. Importantly, we use the same routers without any retraining, and responses from Claude 3 Opus and Llama 3 8B are not present in the training data.

<br />
<img src="/images/blog/routellm/mt-bench-claude-llama.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 45%"></img>

<p style="color:gray; text-align: center;">Figure 7: Router performance on MT Bench when routed to Claude 3 Opus and Llama 3 8B.</p>

Even when the model pair is replaced, we observe strong results across all existing routers on MT Bench, with performance comparable to our original model pair. These results suggest that our routers have learned some common characteristics of problems that can distinguish between strong and weak models, which generalize to new strong and weak model pairs without additional training.

## Conclusion

Our results demonstrate the ability of these routers to achieve significant cost savings while maintaining a high quality of responses across a variety of benchmarks. Our results also highlight the effectiveness of data augmentation and benchmark-dataset similarity scores in improving routing performance using only a small amount of data, offering a scalable path towards improving routing performance for real-world use cases.

Based on our learnings from this research, we have released an open-source framework for serving and evaluating routers on [GitHub](https://github.com/lm-sys/RouteLLM), which we believe is the first framework of its kind supporting LLM routers. We are also releasing all the routers and datasets we have trained on [HuggingFace](https://huggingface.co/routellm) for public use.

We are excited to see what you build on top of this! Please let us know if you face any issues or have any suggestions. For the full details, please refer to our [arXiv](https://arxiv.org/abs/2406.18665) paper.

## Acknowledgements
				
We are grateful to Tyler Griggs for his valuable feedback on this post.
			
## Citations

```
@misc{ong2024routellmlearningroutellms,
      title={RouteLLM: Learning to Route LLMs with Preference Data}, 
      author={Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
      year={2024},
      eprint={2406.18665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.18665}, 
}

@misc{chiang2024chatbot,
    title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
    author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
    year={2024},
    eprint={2403.04132},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{ding2024hybridllmcostefficientqualityaware,
      title={Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing}, 
      author={Dujian Ding and Ankur Mallick and Chi Wang and Robert Sim and Subhabrata Mukherjee and Victor Ruhle and Laks V. S. Lakshmanan and Ahmed Hassan Awadallah},
      year={2024},
      eprint={2404.14618},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.14618}, 
}
```
