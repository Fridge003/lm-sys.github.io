---
title: "What’s up with Llama 3? Arena battle analysis"
author: "Lisa Dunlap, Evan Frick, Tianle Li, Isaac Ong, Joseph E. Gonzalez Wei-Lin Chiang"
date: "May 2, 2024"
previewImg: /images/blog/llama3/llama3_blog_cover.png
---

On April 18th, Meta released Llama 3, their newest open-source large language model. Since then,  Lama 3-70B has quickly risen to the top of the English leaderboard with over 50,000 battles (and counting). This remarkable achievement by Meta is excellent news for the open-source community. However, there has been a lot of speculation about why Llama 3 performed so well in the Chatbot Arena.  Is it just “friendlier” and better at markdown formatting or is it also better at answering questions? These are the ingredients of great research and in  this blog post, we aim to provide more insight into why users rank Llama 3-70b on par with top-ranked models like GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus.

We investigate the following:
1. What types of prompts are users asking? Do users prefer Llama 3 on certain types of prompts? 
2. How hard are these prompts? Does the ranking change if the prompts are easier/harder?
3. Are certain users or prompts overrepresented? Do duplicate prompts or rankings from a small number of users affect the win-rate?
4. Does Llama 3 have qualitative differences which make users like it more?

We focus on battles consisting of llama-3-70b against 5 top-ranked models and reach the following conclusions:
1. Llama 3 beats other top-ranking models on open-ended writing and create problems and loses on more close-ended math and coding problems
2. As prompts get harder, Llama 3’s win-rate against top-tier models drops significantly
3, Deduplication or outliers do not significantly affect the win-rate
4. Qualitatively, Llama 3’s outputs are friendlier and more conversational than other models, and these traits appear more often in battles that llama 3 wins


<img src="/images/blog/llama3/topic_win_rate.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%"></img>
<p style="color:gray; text-align: center;">Figure 1. LLama3-70b win-rate(excluding ties) against top 5 models across prompt topics. * denotes that the category contains less than 50 battles.</p>



## Analyzing win-rate across different types of prompts

**Topic Analysis.** We utilize an LLM labeler (Llama 3-70b) to categorize user prompts into a pre-established taxonomy of topics and visualize the win-rate of Llama3-70b against the other top models. We see that Llama’s win rate is highest for open-ended and creative tasks like brainstorming and writing, and lowest for more close-ended technical tasks like math and translation. Interestingly, Llama 3 achieves the highest win-rate over data processing tasks which mainly consist of parsing and dataframe operations, but as this category has only 19 examples this remains inconclusive. 

**Win-rate VS prompt difficulty.** We employ our [recently released pipeline](https://lmsys.org/blog/2024-04-19-arena-hard/) which scores the difficulty of prompts to determine how Llama 3 compares to the other top models as prompts get harder. We define a set of  "hardness" criteria and use GPT-4-turbo to annotate each prompt from 0 to 7 to indicate how many of these criteria are satisfied (higher score indicates a harder prompt). Our 7 criteria are:

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <tr style="background-color: black; color: white;">
    <!-- <th style="border: 1px solid black; padding: 10px; text-align: left;">7 Key "Hardness" Criteria</th> -->
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>1. Specificity:</strong> Does the prompt ask for a specific output?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>2. Domain Knowledge:</strong> Does the prompt cover one or more specific domains?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>3. Complexity:</strong> Does the prompt have multiple levels of reasoning, components, or variables?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>4. Problem-Solving:</strong> Does the prompt directly involve the AI to demonstrate active problem-solving skills?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>5. Creativity:</strong> Does the prompt involve a level of creativity in approaching the problem?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>6. Technical Accuracy:</strong> Does the prompt require technical accuracy in the response?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>7. Real-world Application:</strong> Does the prompt relate to real-world applications?</td>
  </tr>
</table>

We score 1000 battles against the top 3 models on the leaderboard and plot win-rate VS prompt score in Figure X. We observe a significant drop in Llama 3's performance compared to the other top models, from high 50% win-rate to low 40%. We conclude that as more of these ``hardness'' criteria are met, Llama 3's win rate drop rapidly compared to other models. Notw that these criteria may not be exhaustive, see [the blog](https://lmsys.org/blog/2024-04-19-arena-hard/) for further discussion. 

We can further analyze to a more granular level which types of prompts affect win-rate by fitting a decision tree on the 7 binary columns representing if a given prompt has satisfied each of the criteria above. From this decision tree we can segment prompts into criteria subsets such that Llama3-70b-Instruct performs very well or very poorly. The tree shown in Figure 3 shows us which subsets change the model’s win-rate the most when conditioned on.

<img src="/images/blog/llama3/dtree.svg" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 100%"></img>
<p style="color:gray; text-align: center;">Figure 3. Llama3-70b-Instruct win-rate conditioned on hierarchical prompt criteria subsets as fitted using a standard decision tree algorithm. *English battles against top models (claude-3-opus-20240229, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-turbo-2024-04-09, gemini-1.5-pro-api-0409-preview).</p>

Shown in Figure 3, the first thing to notice is that “Specificity” is the root node of the tree, suggesting that this criteria most immediately divides Llama3-70b-Instruct’s performance into its strengths and weaknesses.  It supports our initial findings above that Llama3-70b-Instruct is stronger on open-ended tasks rather than more closed-ended tasks.  We can traverse further down the tree in Figure 3 and see that Llama3-70b-Instruct is quite strong on open-ended creative prompts (see the blue path), reaching around a 60% win-rate against these top models.  Emperically, these types of prompts are often writing and brainstorming style prompts. For example two prompts where Llama-3-70B-Instruct won are: "Write the first chapter of a novel." and "Could you provide two story suggestions for children that promote altruism?	".  On the other hand, following the orange path in Figure 3, we can notice that Llama3-70b-Instruct has a lower win-rate against top models when answering close-ended, non-real-world, reasoning-based prompts.  These prompts are often logic puzzles and math word word problems. Two examples where Llama-3-70B-Instruct won are: "123x = -4x * 2 - 65" and "There are two ducks in front of a duck, two ducks behind a duck and a duck in the middle. How many ducks are there?"

## The effect of overrepresented prompts and judges

**Effect of duplicate prompts.** Using fuzzy string matching, we find that ~9% (6658/7327) of the user prompts in battles between Llama 3 and the other top models are duplicates, and show in Table X that deduplication does not significantly affect Llama3 win-rate. 

<style>
th {text-align: left, text-weight: bold}
td {text-align: left}
</style>


<br>
<p style="color:gray; text-align: center;">Table 1: Llama 3-70b battle stats.</p>
<table style="display: flex; justify-content: center;">
<tbody>
<tr>
<th>Model</th> <th># battles</th> <th># battles no tie</th> <th># battles (dedup, no tie)</th> <th>Llama 3 win rate</th> <th>Llama 3 win rate (dedup, no tie)</th>
</tr>
<tr>
<td>Claude 3 Opus</td> <td>1959</td> <td>1328</td> <td>1171</td> <td>51.28%</td> <td>51.58%</td>
</tr>
<tr>
<td>Gemini 1.5</td> <td>2413</td> <td>1620</td> <td>1437</td> <td>50.06%</td> <td>49.48%</td>
</tr>
<tr>
<td>GPT-4 0125</td> <td>1271</td> <td>881</td> <td>779</td> <td>48.58%</td> <td>49.04%</td>
</tr>
<tr>
<td>GPT-4 1106</td> <td>526</td> <td>349</td> <td>307</td> <td>50.72%</td> <td>52.12%</td>
</tr>
<tr>
<td>GPT-4-Turbo</td> <td>2097</td> <td>1437</td> <td>1287</td> <td>47.74%</td> <td>47.73%</td>
</tr>
</tbody>
</table>


**User analysis.** First we consider some basic judging statistics to check that judging behavior is similar between Claude-3-opus-20240229 and Llama 3-70b.

As seen in Figure 4, there is no significant strange behavior as the number of votes per judge increases.  In fact, it seems that as the number of votes per judge increases, the judge’s own battles roughly approach the actual global win rate.

<div style="display: flex; justify-content: center; align-items: center;">
  <div style="text-align: center;">
    <img src="/images/blog/llama3/llama_num_judge.png" alt="Image 1" style="width: 40%; height: auto; margin-right: 1%;">
    <img src="/images/blog/llama3/claude_num_judge.png" alt="Image 2" style="width: 40%; height: auto;">
    <p style="color:gray; text-align: center;">Fig 4: Average Winrate for each judge vs Number of judgments made by each judge.</p>
  </div>
</div>


<br>
<p style="color:gray; text-align: center;">Table 2. Detailed Engagement Metrics for LLMs (Timeframe: April 24 - May 1, 2023). The latest and detailed version <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" target="_blank">here</a>.</p>
<table style="display: flex; justify-content: center;">
<tbody>
<tr>
<th>Model</th> <th>Battles</th> <th>Unique Judges</th> <th>Mean Votes per Judge</th> <th>Median Votes per Judge</th> <th>Max Votes per Judge</th> <th>Mean Judge Age</th> <th>Median Judge Age</th> <th>Max Judge Age</th>
</tr>
<tr>
<td>Llama3-70B-Instruct</td> <td>12,719</td> <td>7,591</td> <td>1.68</td> <td>1</td> <td>65</td> <td>1 hr 10 min</td> <td>2 hr 12 min</td> <td>2 days</td>
</tr>
<tr>
<td>Claude-3-opus-20240229</td> <td>68,656</td> <td>48,570</td> <td>1.41</td> <td>1</td> <td>73</td> <td>1 hr 57 min</td> <td>1 hr 55 min</td> <td>3 days</td>
</tr>
<tr>
<td>All Models All Time</td> <td>749,205</td> <td>316,372</td> <td>2.37</td> <td>1</td> <td>591</td> <td>8 hr 27 min</td> <td>2 hr 23 min</td> <td>295 days</td>
</tr>
</tbody>
</table>
<p>*Judge age is defined as rating_date - min_rating_date(ip_address)</p>

In order to limit the impact of user’s that vote many times we can take the mean of each judge’s win rate, thereby bounding the impact of each individual judge. In this case, we find this stratified win rate in Table 2 is still very similar to the original winrate, suggesting that very active judges are not skewing the result.


<br>
<p style="color:gray; text-align: center;">Table 2. Model Win Rates (Timeframe: April 24 - May 1, 2023). The latest and detailed version <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" target="_blank">here</a>. Note that ties are counted as 0.5, with wins and losses as 1 and 0, respectively.</p>
<table style="display: flex; justify-content: center;">
<tbody>
<tr>
<th>Model</th> <th>Win rate</th> <th>Stratified Winrate</th>
</tr>
<tr>
<td>Llama3-70B-Instruct</td> <td>0.541</td> <td>0.543</td>
</tr>
<tr>
<td>Claude-3-opus-20240229</td> <td>0.619</td> <td>0.621</td>
</tr>
</tbody>
</table>

## Qualitative differences between Llama3 outputs VS other models
From qualitative analysis of outputs between Llama3 and other models, we observe that Llama3 outputs are often more excited, positive, conversational, and friendly than other models.  

**Measuring sentiment.** To measure excitement, we assign a binary label to each output based on the presence of an exclamation point. For positivity, friendliness, and conversationality, we use GPT-3.5 as a judge to rate each output on a scale of 1-5. In a given battle, Llama 3 outputs are labeled as more excited, positive, conversational, or friendly if their score is higher than the opponent's. Figure 5 displays the distribution of these qualities across models, revealing that Llama 3 outputs generally exhibit higher levels of excitement, positivity, friendliness, and conversationality compared to their opponents.

<img src="/images/blog/llama3/llama_sentiment_distribution.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%"></img>
<p style="color:gray; text-align: center;">Figure 5: Sentiment Distribution.</p>

**Is sentiment related to win-rate?** Figure 6 compares the sentiment qualities of Llama 3's outputs in battles it wins versus those it loses. We see that all traits appear more in winning battles and less in losing battles, but this difference is relatively small, especially for positivity and friendliness. This suggests that while these traits might play a role in competitive success, their influence requires further exploration for more definitive insights.

<img src="/images/blog/llama3/sentiment_win_rate.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%"></img>
<p style="color:gray; text-align: center;">Figure 6: Llama 3-sentiment VS win-rate.</p>

## Conclusion

From the beginning, our mission has been to advance LLM development and understanding. In the past we have focused on high-level ranking and benchmark design.  Moving forward we hope to extend the analysis here and conduct more in depth analysis into changes in human preference as well as model behavior.  