# Why is Llama 3 so good? Analyzing model performance and behavioral differences
On April 18th, Meta released Llama 3, their newest open-source large language model. Since then, the Chatbot Arena has seen over 15,000 battles against Llama 3-70B alone, and it has quickly risen to the top of the English leaderboard. This remarkable achievement by Meta is excellent news for the open-source community, and we believe this remarkable result deserves further analysis. In this blog post, we aim to provide more insight into why users often prefer Llama 3-70b to top-ranked models like GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus.

We investigate the following:
1. What types of questions are users asking? Do users prefer Llama 3 on certain types of questions? 
2. How hard are these questions? Does the ranking change if the questions are easier/harder?
3. Are certain users or questions overrepresented? Do duplicate questions or rankings from a small number of users affect the win-rate?
4. Does Llama 3 have qualitative differences which make users like it more?

We focus on battles consisting of llama-3-70b against 5 top-ranked models and reach the following conclusions:
1. Llama 3 beats other top-ranking models on open-ended writing problems and loses on more objective math and coding problems
2. As prompts get harder*, Llama 3’s win-rate drops significantly. 
3, Deduplication or outliers do not significantly affect the win-rate
4. Qualitatively, llama 3’s outputs are pore positive, friendlier and more conversational than other models
5. There is a correlation between llama being more conversational and llama winning

<img src="/images/blog/llama3/topic_winrate.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img>
<p style="color:gray; text-align: center;">Figure 1: Llama 3-70b win-rate against top 5 models across topic clusters.</p>



## Analyzing win-rate across different types of questions

**Topic Analysis.** We perform topic clustering on 10k user prompts from battles with Llama 3-70b to get a further understanding of the types of questions being asked (see arena-hard for further explanation of the topic clustering pipeline). In Figure X we see the win rate of Llama 3 against the other top5 models across different topic clusters of user prompts. We see that llamas win-rate is highest for clusters which contain open-ended writing questions while its lowest win rate happens for more objective tasks like C++ programming, as well as long context writing prompts. We see this conclusion proved further in the following section. 

**Win-rate VS prompt difficulty.** We employ our recently released pipeline which scores the difficulty of prompts to determine how Llama 3 compares to the other top models as prompts get harder. We define a set of  ``hardness' ' criteria and use GPT-4-turbo  to annotate each prompt from 0 to 7 to indicate how many of these criteria are satisfied (higher score indicates a harder prompt). Our 7 criteria are:

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <tr style="background-color: black; color: white;">
    <th style="border: 1px solid black; padding: 10px; text-align: left;">Table 2: 7 Key "Hardness" Criteria</th>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>1. Specificity:</strong> Does the prompt ask for a specific output?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>2. Domain Knowledge:</strong> Does the prompt cover one or more specific domains?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>3. Complexity:</strong> Does the prompt have multiple levels of reasoning, components, or variables?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>4. Problem-Solving:</strong> Does the prompt directly involve the AI to demonstrate active problem-solving skills?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>5. Creativity:</strong> Does the prompt involve a level of creativity in approaching the problem?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>6. Technical Accuracy:</strong> Does the prompt require technical accuracy in the response?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>7. Real-world Application:</strong> Does the prompt relate to real-world applications?</td>
  </tr>
</table>

We score 1000 battles against the top 3 models on the leaderboard and plot win-rate VS prompt score in Figure X. We observe a significant drop in Llama 3's performance compared to the other top models, from high 50% win-rate to low 40%. We conclude that as prompts get harder, users prefer Llama 3 outputs much less than other models[^1].

[^1]: this scoring system may not capture all prompts which the average user the hardness criteria we define are not exhaustive and often allocate high scores to more objective problems like math and coding questions.


## The effect of overrepresented questions and judges

**Effect of duplicate questions.** An interesting finding from our topic clustering was the existence of a  "Sentences Ending with “Apple” cluster. From manual inspection we confirm that this cluster does indeed contain many variations on the question "write 10 sentences that end with the word “apple". This raises questions on if this repeated question is largely contributing to the overall ranking, which we explore in the following section. Using fuzzy string matching, we find that 88% (1730/1975) of the user prompts in battles between Llama 3 and the other top models are unique, and show in Table X that deduplication does not significantly affect Llama 3 win-rate, and only changes 

<style>
th {text-align: left}
td {text-align: left}
</style>

<br>
<p style="color:gray; text-align: center;">Table X: Llama 3-70b battle counts and win rates against the top 5 models in the arena with and without duplication.</p>
<table style="display: flex; justify-content: center;" align="left" >
<tbody>
<tr>
<th>Model</th> <th># battles</th> <th># battles (deduplicated)</th> <th>Llama 3 win rate</th> <th>Llama 3 win rate (deduplicated)</th>
</tr>
<tr>
<td>GPT-4-Turbo</td> <td>634</td> <td>559</td> <td>47.31%</td> <td>46.35%</td>
</tr>
<tr>
<td>GPT-4 1106</td> <td>116</td> <td>112</td> <td>50.60%</td> <td>51.39%</td>
</tr>
<tr>
<td>GPT-4 0125</td> <td>548</td> <td>480</td> <td>50.21%</td> <td>50.39%</td>
</tr>
<tr>
<td>Claude 3 Opus</td> <td>478</td> <td>416</td> <td>52.82%</td> <td>52.80%</td>
</tr>
<tr>
<td>Gemini 1.5</td> <td>199</td> <td>163</td> <td>58.46%</td> <td>57.12%</td>
</tr>
</tbody>
</table>


**User analysis.** First we consider some basic judging statistics to check that judging behavior is similar between Claude-3-opus-20240229 and Llama 3-70b.

Visually, there is no significant strange behavior as the number of votes per judge increases.  In fact, it seems that as the number of votes per judge increases, the judge’s own battles roughly approach the actual global win rate.

In order to limit the impact of user’s that vote many times we can take the mean of each judge’s win rate, thereby bounding the impact of each individual judge. In this case, we find this stratified win rate is still very similar to the original winrate, suggesting that very active judges are not skewing the result.


<br>
<p style="color:gray; text-align: center;">Table 4. Detailed Engagement Metrics for LLMs (Timeframe: April 24 - May 1, 2023). The latest and detailed version <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" target="_blank">here</a>.</p>
<table style="display: flex; justify-content: center;" align="left" >
<tbody>
<tr>
<th>Model</th> <th>Battles</th> <th>Unique Judges</th> <th>Mean Votes per Judge</th> <th>Median Votes per Judge</th> <th>Max Votes per Judge</th> <th>Mean Judge Age</th> <th>Median Judge Age</th> <th>Max Judge Age</th>
</tr>
<tr>
<td>Llama3-70B-Instruct</td> <td>12,719</td> <td>7,591</td> <td>1.68</td> <td>1</td> <td>65</td> <td>1 hr 10 min</td> <td>2 hr 12 min</td> <td>2 days</td>
</tr>
<tr>
<td>Claude-3-opus-20240229</td> <td>68,656</td> <td>48,570</td> <td>1.41</td> <td>1</td> <td>73</td> <td>1 hr 57 min</td> <td>1 hr 55 min</td> <td>3 days</td>
</tr>
<tr>
<td>All Models All Time</td> <td>749,205</td> <td>316,372</td> <td>2.37</td> <td>1</td> <td>591</td> <td>8 hr 27 min</td> <td>2 hr 23 min</td> <td>295 days</td>
</tr>
</tbody>
</table>
<p>*Judge age is defined as rating_date - min_rating_date(ip_address)</p>

## Qualitative differences between Llama3 outputs VS other models
From qualitative analysis of outputs between Llama3 and other models, we observe that Llama3 outputs are often more excited, positive, conversational, and friendly than other models.  

**Measuring sentiment.** To measure excitement, we assign a binary label to each output based on the presence of an exclamation point. For positivity, friendliness, and conversationality, we use GPT-3.5 as a judge to rate each output on a scale of 1-5. In a given battle, Llama 3 outputs are labeled as more excited, positive, conversational, or friendly if their score is higher than the opponent's. Figure X displays the distribution of these qualities across models, revealing that Llama 3 outputs generally exhibit higher levels of excitement, positivity, friendliness, and conversationality compared to their opponents.

<img src="/images/blog/llama3/llama_sentiment_distribution.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img>
<p style="color:gray; text-align: center;">Figure X: Sentiment Distribution.</p>

**Is sentiment related to win-rate?** Figure X compares the sentiment qualities of Llama 3's outputs in battles it wins versus those it loses. The results suggest that winning outputs tend to be more conversational and slightly friendlier, indicating that engaging and positive communication may contribute to success in these competitive scenarios.
To further investigate this correlation, we used logistic regression to analyze how differences in exclamation, positivity, friendliness, and conversationality between outputs affect user preference. Conversationality had the greatest impact, increasing a model's odds of winning by 27.5% when it was more conversational than its opponent. Friendliness and excitement also positively influenced preferences, raising the odds by 15.9% and 22.0%, respectively. Positivity showed no statistically significant correlation with win-rate. However, the low pseudo R-squared values across all attributes suggest that while these factors are statistically significant, they only partially explain the variance in model selection, implying that other unmeasured factors also play important roles in shaping user choices.

<img src="/images/blog/llama3/sentiment_win_rate.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img>
<p style="color:gray; text-align: center;">Figure X: Llama 3-sentiment VS win-rate.</p>